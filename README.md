# Moore's Law in the Era of AI: Hardware Trends for Strategic Advantage

## Project Overview

Moore's law is the observation made by Gordon Moore in 1965, it states that the number of transistors on an integrated circuit will double every two years with minimal rise in cost. Semiconductors are one such circuit and they are critical to AI as it provides the necessary hardware for complex computations. This project seeks to determine if Moore's Law will continue to hold true as AI advances. While the law has proven correct for the past five decades, some researchers believe it could be hitting a plateau[X https://www.electrochem.org/moores-law-the-beginnings]. With the increased of AI technology, however, that could trump the semiconductor industry.

## Business Problem

Based on the study, the following are business problems to consider:

- Is Moore’s Law still a useful predictor of progress in the AI hardware era?
- If Moore’s Law slows or breaks down, how should businesses and industries adjust their expectations, investments, and strategies?
- Is the future of scaling more about software, architecture, or energy efficiency than pure transistor count?
- Are there other factors that could be slowing down or incresing progress?

## Datasets Used

Primary: (https://chip-dataset.vercel.app/) 
Secondary: (https://www.techpowerup.com/)

## Data Dictionary

| Variable                         | Type   | Description                                                                                                                                                         |
| -------------------------------- | ------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Product                          | string | Name of the product.                                                                                                                                                |
| Type                             | string | Graphics Processing Unit (GPU) or Central Processing Unit (CPU).                                                                                                    |
| Release Date                     | date   | Date the product was released.                                                                                                                                      |
| Process Size (nano meters)       | int    | Physical size of the components that make up a chip.                                                                                                                |
| Thermal Design Power TDP (Watts) | int    | The maximum amount of heat that a computer component (like a CPU or GPU) can generate and that its cooling system is designed to dissipate during normal operation. |
| Die Size (mm^2)                  | int    | The physical surface area of the individual chip.                                                                                                                   |
| Transistors (million)            | int    | A semiconductor device used to amplify or swtich electrical signals.                                                                                                |
| Freq (GHz)                       | int    | A measure for for how quickly an electronic device oeprates.                                                                                                        |
| Foundry                          | string | The factory that produces the device.                                                                                                                               |
| Vendor                           | string | The vendor of the device.                                                                                                                                           |
| FP16 GFLOPS                      | float  | Giga floating-point operations per second a processor or GPU can perform using FP16 (16-bit floating point) precision.                                              |
| FP32 GFLOPS                      | float  | Giga floating-point operations per second a processor or GPU can perform using FP16 (32-bit floating point) precision.                                              |
| FP64 GFLOPS                      | float  | Giga floating-point operations per second a processor or GPU can perform using FP16 (64-bit floating point) precision.                                              |

## Methods

TO DO

## Key Findings

TO DO

## Business Implications

TO DO

## Reproduction

TO DO

## References

TO DO
